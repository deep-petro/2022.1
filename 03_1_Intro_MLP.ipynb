{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_1_Intro_MLP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NG-mVsVuE0if",
        "3f-q6bCNzEmx"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG-mVsVuE0if"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports, funções, downloads e instalação do Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEHmMCjR4PJw"
      },
      "source": [
        "# Basic imports.\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from torchvision import models\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from skimage import io\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQa4-lUw7Rmp"
      },
      "source": [
        "## Casting para o dispositivo correto\n",
        "\n",
        "Como usaremos processamento vetorial principalmente em GPUs para aprendizado profundo, primeiramente é possível verificar se há uma GPU disponível com o trecho de código abaixo, armazenando os tensores nos dispositivos apropriados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX0bBEM863sY"
      },
      "source": [
        "# Checking if GPU/CUDA is available.\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x5UK0uib2tk"
      },
      "source": [
        "# Intro MLP\n",
        "\n",
        "## O perceptron e a camada `nn.Linear`\n",
        "\n",
        "A camada Linear do Pytorch ([nn.Linear](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear)) é responsável por aplicar uma transformação linear no dado de entrada. Esta camada recebe como parâmetro a dimensão (número de *features*) da entrada e da saída (que na verdade, representa o número de neurônios dessa camada). Por padrão o bias já é incluído. **Um** perceptron pode ser facilmente representado como a seguir, desconsiderando a função de ativação:\n",
        "\n",
        "```\n",
        "perceptron = nn.Linear(in_dimension, 1)\n",
        "```\n",
        "Mas de uma forma geral, uma camada Linear com diversas *features* de entrada e diversas *features* de saída pode ser representada como:\n",
        "```\n",
        "nn.Linear(in_features, out_features)\n",
        "```\n",
        "![](./figs/nn_linear.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlQA_vtGg8bf"
      },
      "source": [
        "linear = nn.Linear(2,3)\n",
        "print(linear)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AhNyLrLmFcT"
      },
      "source": [
        "Como é possível ver no código abaixo, o Pytorch já inicia os pesos da camada aleatoriamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLOlOhQVmPuj"
      },
      "source": [
        "for p in linear.parameters():\n",
        "  print(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAEaJGtDoZD7"
      },
      "source": [
        "O **forward** consiste em passar seu dado de entrada pela rede, gerando um resultado ao final. Considerando a camada linear instanciada anteriormente, o resultado do forward é o mesmo do somatório da multiplicação de seus pesos pelas respectivas entradas juntamente com o bias:\n",
        "\n",
        "w1\\*x1 + w2\\*x2 + ... + wn\\*xn + b\n",
        "\n",
        "No Pytorch, realizamos o **forward** chamando a função onde nossa rede/modelo está instanciada, conforme exemplo abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibb8t7zpmpUI"
      },
      "source": [
        "perceptron = nn.Linear(2,1)\n",
        "X = torch.FloatTensor([2,3]) # dado de entrada de exemplo considerando o perceptron definido como nn.Linear(2,1)\n",
        "print('Pytorch: ', perceptron(X))\n",
        "\n",
        "# acessamos os pesos do modelo com .weight e o bias com .bias\n",
        "print('Manual: ', torch.mul(X, perceptron.weight).sum() + perceptron.bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8LMX6OMrEbw"
      },
      "source": [
        "## Exemplo Perceptron simples\n",
        "\n",
        "O código abaixo cria um perceptron simples usando `nn.Linear` e implemente o fluxo de treinamento para esse preceptron, ou seja, faz o forward nessa camada, calcula a loss, e otimiza a camada. Invista um pouco de tempo para entender a célula abaixo pois usaremos essa ideia para implementar a função de treino mais a frente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPJ9s5ckoA5T"
      },
      "source": [
        "def loss_fn(predict, label):  # definindo a loss\n",
        "    return torch.pow(label - predict, 2)\n",
        "\n",
        "perceptron = nn.Linear(1,1) # Camada linear com 1 feature de entrada (mais o bias) e uma de saída\n",
        "perceptron.to(device) # casting do perceptron para GPU\n",
        "learning_rate = 0.01\n",
        "print('Parametros iniciais: ', list(perceptron.parameters()))\n",
        "\n",
        "dataset = [] # dados de exemplo que seguem a função y = 2x+3\n",
        "for x in range(10):\n",
        "    dataset.append((x, 2*x+3))\n",
        "\n",
        "for epoch in range(101):\n",
        "    epoch_loss = 0\n",
        "    for iteration, data in enumerate(dataset):\n",
        "        X, y = data\n",
        "        X, y = torch.FloatTensor([X]).to(device), torch.FloatTensor([y]).to(device) # conversão para Tensor\n",
        "  \n",
        "        y_pred = perceptron(X)  # forward\n",
        "        loss = loss_fn(y_pred, y)  # calcula a loss\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            for param in perceptron.parameters():\n",
        "                param -= learning_rate * param.grad  # atualização dos parametros (pesos e bias) com base no gradiente\n",
        "                param.grad.zero_()  # resetando o gradiente\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch {} - loss: {}\".format(epoch, epoch_loss))\n",
        "print('Parametros finais: ', list(perceptron.parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9uRk8mAwGIB"
      },
      "source": [
        "print(perceptron(torch.FloatTensor([20]).to(device))) # forward do valor 20 para conferir resultado, saida deve ser aproximadamente = 2x+3 = 2*20+3 = 43"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f-q6bCNzEmx"
      },
      "source": [
        "## O módulo `nn.Sequential`\n",
        "\n",
        "Na prática, criaremos redes com diversas camadas. O bloco `nn.Sequential` permite agrupar as camadas de forma sequencial para que o forward seja realizado na ordem desejada. Veja um exemplo para um *Multilayer Perceptron (MLP)* abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCK_OkqCzdUW"
      },
      "source": [
        "in_features = 28\n",
        "hidden_1 = 64\n",
        "hidden_2 = 32\n",
        "out_features = 8\n",
        "\n",
        "MLP = nn.Sequential(nn.Linear(in_features, hidden_1), nn.ReLU(), \n",
        "                    nn.Linear(hidden_1, hidden_2), nn.ReLU(), \n",
        "                    nn.Linear(hidden_2, out_features))\n",
        "print(MLP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkhjCepK0kjJ"
      },
      "source": [
        "test_data = torch.randn((10,28)) # 10 dados de input aleatórios com 28 features\n",
        "output = MLP(test_data) # forward da rede\n",
        "print(output.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9LoXL0cUYMT"
      },
      "source": [
        "## Sua vez\r\n",
        "\r\n",
        "Vamos agora treinar um MLP simples em dados aleatórios. A célula abaixo define as dimensões de entrada e saída e gera os dados aleatórios."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi3Zh8fQ4X_3"
      },
      "source": [
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random Tensors to hold inputs and outputs\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "# Casting tensors to the appropriate device.\n",
        "x = x.to(device)\n",
        "y = y.to(device)\n",
        "\n",
        "# Printing sizes of tensors.\n",
        "print('x: ', x.size())\n",
        "print('y: ', y.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMDN1viW-0Eg"
      },
      "source": [
        "Modifique o código abaixo para criar um módulo `nn.Sequential` de nome **model** que representa um MLP com, pelo menos, uma camada escondida (seguindo os valores N, D_in, H e D_out definidos anteriormente), **usando um ReLU como função de ativação entre as camadas**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYO7HWC29Ahy"
      },
      "source": [
        "# Use the nn package to define our model.\n",
        "model = # ...\n",
        "\n",
        "model.to(device)  # sempre eh necessario fazer o casting da rede para joga-la para GPU\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWrn_MUHBz7o"
      },
      "source": [
        "Abaixo, definimos uma loss e um otimizador usando o Torch. Não se preocupem como isso agora, pois iremos ver em detalhes como definir e usar diferentes losses and otimizadores com o Torch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oQ2T8jm9BE9"
      },
      "source": [
        "# Use the nn package to define our loss function.\n",
        "loss_mse = nn.MSELoss(reduction='sum').to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa5DcYBf82iD"
      },
      "source": [
        "# Use the optim package to define an Optimizer that will update the weights of\n",
        "# the model for us. Here we will use SGD; the optim package contains many other\n",
        "# optimization algorithms. The first argument tells the\n",
        "# optimizer which Tensors it should update.\n",
        "learning_rate = 1e-4\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPQtOnNr-kAG"
      },
      "source": [
        "Implemente abaixo a forward e cálculo da loss como feito anteriormente. Estude essa função, pois usaremos esse fluxo de treino mais a frente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsMFRIDv80I3"
      },
      "source": [
        "# Creating list of losses for each epoch.\n",
        "loss_list = []\n",
        "\n",
        "# Iterating over epochs.\n",
        "for epoch in range(500):\n",
        "\n",
        "    ###########\n",
        "    # Implemente sua solução aqui\n",
        "    ###########\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print('Epoch ' + str(epoch + 1) + ': loss = ' + str(loss.item()))\n",
        "    \n",
        "    # Updating list of losses for printing.\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "    # Before the backward pass, use the optimizer object to zero all of the\n",
        "    # gradients for the variables it will update (which are the learnable\n",
        "    # weights of the model). This is because by default, gradients are\n",
        "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
        "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to model\n",
        "    # parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Calling the step function on an Optimizer makes an update to its\n",
        "    # parameters\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toQyqq98-68X"
      },
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
        "\n",
        "ax.plot(np.asarray(loss_list))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfb4zBjO0Lua"
      },
      "source": [
        "Informação sobre outras camadas lineares, como nn.Bilinear e nn.Identity, podem ser vistas na documentação: https://pytorch.org/docs/stable/nn.html#linear-layers"
      ]
    }
  ]
}